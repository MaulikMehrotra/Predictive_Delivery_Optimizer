{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "2b4d502b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading CSVs...\n",
      "Merging tables on Order_ID...\n",
      "Cleaning and filling missing values...\n",
      "Creating derived features...\n",
      "Using features: ['Distance_KM', 'Traffic_Delay_Minutes', 'Weather_Impact', 'Order_Value_INR', 'Delivery_Cost_INR', 'Priority', 'Carrier', 'Customer_Segment', 'Product_Category', 'Fuel_Cost_per_KM', 'Delivery_Efficiency', 'Revenue_per_KM']\n",
      "Encoding categorical variables...\n",
      "Training RandomForestClassifier...\n",
      "\n",
      "Model Accuracy: 0.7750\n",
      "\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.79      0.88      0.84        26\n",
      "           1       0.73      0.57      0.64        14\n",
      "\n",
      "    accuracy                           0.78        40\n",
      "   macro avg       0.76      0.73      0.74        40\n",
      "weighted avg       0.77      0.78      0.77        40\n",
      "\n",
      "\n",
      "Confusion Matrix:\n",
      "[[23  3]\n",
      " [ 6  8]]\n",
      "Feature importance plot saved to feature_importances.png\n",
      "Saving processed data and model artifacts...\n",
      "Saved processed dataset -> processed_data.csv\n",
      "Saved model+scaler+meta -> delivery_delay_model.pkl\n",
      "Saved encoders -> encoders.pkl\n",
      "All done.\n"
     ]
    }
   ],
   "source": [
    "# main.py\n",
    "\"\"\"\n",
    "Full data processing + model training pipeline for Predictive Delivery Optimizer.\n",
    "\n",
    "Outputs:\n",
    " - processed_data.csv       # cleaned & feature-engineered dataset\n",
    " - delivery_delay_model.pkl # trained classifier\n",
    " - encoders.pkl             # dict of label encoders used for categorical columns\n",
    "\"\"\"\n",
    "\n",
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelEncoder, StandardScaler\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import classification_report, confusion_matrix, accuracy_score\n",
    "import joblib\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "# -------------------------\n",
    "# 1) CONFIG / FILE PATHS\n",
    "# -------------------------\n",
    "DATA_FILES = {\n",
    "    \"delivery\": \"delivery_performance.csv\",\n",
    "    \"orders\": \"orders.csv\",\n",
    "    \"routes\": \"routes_distance.csv\",\n",
    "    \"vehicles\": \"vehicle_fleet.csv\",  # optional merge if Vehicle_ID present\n",
    "}\n",
    "\n",
    "OUTPUT_PROCESSED = \"processed_data.csv\"\n",
    "OUTPUT_MODEL = \"delivery_delay_model.pkl\"\n",
    "OUTPUT_ENCODERS = \"encoders.pkl\"\n",
    "\n",
    "# -------------------------\n",
    "# 2) LOAD DATAFRAMES\n",
    "# -------------------------\n",
    "def load_csv_safe(path):\n",
    "    if os.path.exists(path):\n",
    "        return pd.read_csv(path)\n",
    "    else:\n",
    "        raise FileNotFoundError(f\"Required file not found: {path}\")\n",
    "\n",
    "print(\"Loading CSVs...\")\n",
    "df_delivery = load_csv_safe(DATA_FILES[\"delivery\"])\n",
    "df_orders = load_csv_safe(DATA_FILES[\"orders\"])\n",
    "df_routes = load_csv_safe(DATA_FILES[\"routes\"])\n",
    "# vehicles file optional (may not join to orders directly)\n",
    "df_vehicles = pd.read_csv(DATA_FILES[\"vehicles\"]) if os.path.exists(DATA_FILES[\"vehicles\"]) else None\n",
    "\n",
    "# -------------------------\n",
    "# 3) MERGE\n",
    "# -------------------------\n",
    "# Primary merge: delivery + orders + routes on Order_ID\n",
    "print(\"Merging tables on Order_ID...\")\n",
    "merged = df_delivery.merge(df_orders, on=\"Order_ID\", how=\"outer\") \\\n",
    "                    .merge(df_routes, on=\"Order_ID\", how=\"outer\")\n",
    "\n",
    "# If vehicle mapping exists in orders (e.g., Vehicle_ID) and vehicles df loaded, merge it.\n",
    "if df_vehicles is not None and \"Vehicle_ID\" in merged.columns and \"Vehicle_ID\" in df_vehicles.columns:\n",
    "    merged = merged.merge(df_vehicles, on=\"Vehicle_ID\", how=\"left\")\n",
    "\n",
    "# -------------------------\n",
    "# 4) BASIC CLEANING\n",
    "# -------------------------\n",
    "print(\"Cleaning and filling missing values...\")\n",
    "# Standardize column names (strip spaces)\n",
    "merged.columns = [c.strip() for c in merged.columns]\n",
    "\n",
    "# Fill important numeric columns with sensible defaults\n",
    "numeric_defaults = {\n",
    "    \"Customer_Rating\": 0,\n",
    "    \"Delivery_Cost_INR\": 0.0,\n",
    "    \"Distance_KM\": 0.0,\n",
    "    \"Fuel_Consumption_L\": 0.0,\n",
    "    \"Traffic_Delay_Minutes\": 0.0,\n",
    "    \"Toll_Charges_INR\": 0.0,\n",
    "    \"Order_Value_INR\": 0.0,\n",
    "    \"Promised_Delivery_Days\": np.nan,   # leave NaN for careful handling\n",
    "    \"Actual_Delivery_Days\": np.nan\n",
    "}\n",
    "for col, val in numeric_defaults.items():\n",
    "    if col in merged.columns:\n",
    "        merged[col] = merged[col].fillna(val)\n",
    "\n",
    "# Fill categorical defaults\n",
    "cat_defaults = {\n",
    "    \"Delivery_Status\": \"Unknown\",\n",
    "    \"Quality_Issue\": \"None\",\n",
    "    \"Priority\": \"Standard\",\n",
    "    \"Customer_Segment\": \"Unknown\",\n",
    "    \"Product_Category\": \"Unknown\",\n",
    "    \"Carrier\": \"Unknown\",\n",
    "    \"Route\": \"Unknown\"\n",
    "}\n",
    "for col, val in cat_defaults.items():\n",
    "    if col in merged.columns:\n",
    "        merged[col] = merged[col].fillna(val)\n",
    "\n",
    "# Drop duplicates\n",
    "merged.drop_duplicates(subset=[\"Order_ID\"], inplace=True)\n",
    "\n",
    "# -------------------------\n",
    "# 5) DERIVED FEATURES\n",
    "# -------------------------\n",
    "print(\"Creating derived features...\")\n",
    "\n",
    "# Delay_Days: calculated from integer day counts (Actual - Promised)\n",
    "if \"Actual_Delivery_Days\" in merged.columns and \"Promised_Delivery_Days\" in merged.columns:\n",
    "    merged[\"Delay_Days\"] = merged[\"Actual_Delivery_Days\"] - merged[\"Promised_Delivery_Days\"]\n",
    "else:\n",
    "    # fallback: if those columns don't exist, set Delay_Days to 0\n",
    "    merged[\"Delay_Days\"] = 0\n",
    "\n",
    "# Binary target: Delayed (1) if Delay_Days > 0 else 0\n",
    "merged[\"Delayed\"] = (merged[\"Delay_Days\"] > 0).astype(int)\n",
    "\n",
    "# Distance, traffic and weather derived features - ensure columns exist\n",
    "if \"Distance_KM\" not in merged.columns and \"Distance\" in merged.columns:\n",
    "    merged[\"Distance_KM\"] = merged[\"Distance\"]\n",
    "\n",
    "# Fuel cost per km (use tolls as proxy if no fuel cost)\n",
    "def safe_div(a, b):\n",
    "    try:\n",
    "        return a / b if b and not np.isnan(b) and b != 0 else 0.0\n",
    "    except Exception:\n",
    "        return 0.0\n",
    "\n",
    "merged[\"Fuel_Cost_per_KM\"] = merged.apply(\n",
    "    lambda r: safe_div(r.get(\"Toll_Charges_INR\", 0.0), r.get(\"Distance_KM\", 0.0)), axis=1\n",
    ")\n",
    "\n",
    "# Delivery efficiency: Distance per actual delivery day (avoid divide by zero)\n",
    "merged[\"Delivery_Efficiency\"] = merged.apply(\n",
    "    lambda r: safe_div(r.get(\"Distance_KM\", 0.0), r.get(\"Actual_Delivery_Days\", 1.0)), axis=1\n",
    ")\n",
    "\n",
    "# Revenue per KM (Order value divided by distance)\n",
    "merged[\"Revenue_per_KM\"] = merged.apply(\n",
    "    lambda r: safe_div(r.get(\"Order_Value_INR\", 0.0), r.get(\"Distance_KM\", 0.0)), axis=1\n",
    ")\n",
    "\n",
    "# Cost efficiency score (higher better)\n",
    "merged[\"Cost_Efficiency_Score\"] = merged.apply(\n",
    "    lambda r: round((r[\"Revenue_per_KM\"] / (r.get(\"Delivery_Cost_INR\", 0.0) + 1)) * 100, 2), axis=1\n",
    ")\n",
    "\n",
    "# Simple satisfaction index: rating penalized by delay\n",
    "merged[\"Satisfaction_Index\"] = merged.get(\"Customer_Rating\", 0) * (1 - (merged[\"Delay_Days\"].fillna(0) / 10))\n",
    "merged[\"Satisfaction_Index\"] = merged[\"Satisfaction_Index\"].clip(lower=0)\n",
    "\n",
    "# -------------------------\n",
    "# 6) FEATURE SELECTION for MODEL\n",
    "# -------------------------\n",
    "# Choose a sensible set of features that likely appear in your files\n",
    "candidate_features = [\n",
    "    \"Distance_KM\", \"Traffic_Delay_Minutes\", \"Weather_Impact\",\n",
    "    \"Order_Value_INR\", \"Delivery_Cost_INR\",\n",
    "    \"Priority\", \"Carrier\", \"Customer_Segment\", \"Product_Category\",\n",
    "    \"Fuel_Cost_per_KM\", \"Delivery_Efficiency\", \"Revenue_per_KM\"\n",
    "]\n",
    "\n",
    "# Keep only features that present in merged\n",
    "features = [c for c in candidate_features if c in merged.columns]\n",
    "print(\"Using features:\", features)\n",
    "\n",
    "# Drop rows where target is missing (shouldn't happen) and where Distance_KM is nan\n",
    "model_df = merged.dropna(subset=[\"Delayed\"])\n",
    "model_df[features] = model_df[features].fillna(0)\n",
    "\n",
    "# -------------------------\n",
    "# 7) ENCODE CATEGORICALS\n",
    "# -------------------------\n",
    "print(\"Encoding categorical variables...\")\n",
    "encoders = {}\n",
    "X = model_df[features].copy()\n",
    "\n",
    "for col in X.select_dtypes(include=[\"object\", \"category\"]).columns:\n",
    "    le = LabelEncoder()\n",
    "    X[col] = X[col].astype(str).fillna(\"Unknown\")\n",
    "    X[col] = le.fit_transform(X[col])\n",
    "    encoders[col] = le\n",
    "\n",
    "y = model_df[\"Delayed\"].astype(int)\n",
    "\n",
    "# -------------------------\n",
    "# 8) SCALE NUMERICS (optional)\n",
    "# -------------------------\n",
    "num_cols = X.select_dtypes(include=[np.number]).columns.tolist()\n",
    "scaler = StandardScaler()\n",
    "X[num_cols] = scaler.fit_transform(X[num_cols])\n",
    "\n",
    "# -------------------------\n",
    "# 9) TRAIN/TEST SPLIT + MODEL\n",
    "# -------------------------\n",
    "print(\"Training RandomForestClassifier...\")\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42, stratify=y)\n",
    "\n",
    "clf = RandomForestClassifier(n_estimators=200, random_state=42, n_jobs=-1)\n",
    "clf.fit(X_train, y_train)\n",
    "\n",
    "# -------------------------\n",
    "# 10) EVALUATION\n",
    "# -------------------------\n",
    "y_pred = clf.predict(X_test)\n",
    "acc = accuracy_score(y_test, y_pred)\n",
    "print(f\"\\nModel Accuracy: {acc:.4f}\")\n",
    "print(\"\\nClassification Report:\")\n",
    "print(classification_report(y_test, y_pred, zero_division=0))\n",
    "print(\"\\nConfusion Matrix:\")\n",
    "print(confusion_matrix(y_test, y_pred))\n",
    "\n",
    "# Save a simple feature importance plot\n",
    "if hasattr(clf, \"feature_importances_\"):\n",
    "    importances = pd.Series(clf.feature_importances_, index=X.columns).sort_values(ascending=True)\n",
    "    plt.figure(figsize=(8, max(4, len(importances) * 0.4)))\n",
    "    importances.plot(kind=\"barh\")\n",
    "    plt.title(\"Feature Importances\")\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(\"feature_importances.png\")\n",
    "    plt.close()\n",
    "    print(\"Feature importance plot saved to feature_importances.png\")\n",
    "\n",
    "# -------------------------\n",
    "# 11) SAVE ARTIFACTS\n",
    "# -------------------------\n",
    "print(\"Saving processed data and model artifacts...\")\n",
    "model_artifacts = {\n",
    "    \"model\": clf,\n",
    "    \"scaler\": scaler,\n",
    "    \"feature_columns\": X.columns.tolist()\n",
    "}\n",
    "joblib.dump(model_artifacts, OUTPUT_MODEL)  # single file with model+scaler+cols\n",
    "joblib.dump(encoders, OUTPUT_ENCODERS)\n",
    "model_df.to_csv(OUTPUT_PROCESSED, index=False)\n",
    "\n",
    "print(f\"Saved processed dataset -> {OUTPUT_PROCESSED}\")\n",
    "print(f\"Saved model+scaler+meta -> {OUTPUT_MODEL}\")\n",
    "print(f\"Saved encoders -> {OUTPUT_ENCODERS}\")\n",
    "print(\"All done.\")\n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
